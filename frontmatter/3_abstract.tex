\begin{abstract}

Augmented Reality displays are a next-generation computing platform that offer unprecedented user experience by seamlessly combining physical and digital content, and could revolutionize the way we communicate, visualize, and interact with digital information.
However, providing a seamless and perceptually realistic experience requires displays capable of presenting photorealistic imagery, and especially, perceptually realistic depth cues, resulting in virtual imagery being presented at any depth and of any opacity. Today's commercial augmented reality displays are far from perceptually realistic because they do not support important depth cues such as mutual occlusion and accommodation, resulting in a transparent image overlaid onto the real-world at a fixed depth. Previous research prototypes fall short by presenting occlusion only for a fixed depth, and by presenting accommodation and defocus-blur only for a narrow depth-range, or with poor depth or spatial resolution. 

To address these challenges, this thesis explores a computational display approach, where the displayâ€™s optics, electronics, and algorithms are co-designed to improve performance or enable new capabilities. In one design, a Volumetric Near-eye Augmented Reality Display was developed to simultaneously present many virtual objects at different depths across a large depth range (15 - 400 cm) without sacrificing spatial resolution, frame rate, or bitdepth. This was accomplished by (1) synchronizing a high-speed Digital Micromirror Device (DMD) projector and a focus-tunable lens to periodically sweep out a volume composed of 280 single-color binary images in front of the user's eye, (2) a new voxel-oriented decomposition algorithm, and (3) per-depth-plane illumination control. In a separate design, for the first time, we demonstrate depth-correct occlusion in optical see-through augmented reality displays. This was accomplished by an optical system composed of two fixed-focus lenses and two focus-tunable lenses to dynamically move the occlusion and virtual image planes in depth, and designing the optics to ensure unit magnification of the see-through real world irrespective of the occlusion or virtual image plane distance. 

Contributions of this thesis include new optical designs, new rendering algorithms, and prototype displays that demonstrate accommodation, defocus blur, and occlusion depth cues over an extended depth-range. 

%Augmented Reality near-eye displays are a next-generation computing platform that offer unprecedented user experiences by seamlessly combining physical and digital content. However, providing a seamless and perceptually realistic experience requires displays capable of presenting photorealistic imagery, and especially, perceptually realistic depth cues, resulting in virtual imagery being presented at any depth and of any opacity. Today's commercial augmented reality displays are far from perceptually realistic because they do not support important depth cues such as accommodation and mutual occlusion, resulting in a transparent image overlaid onto the real-world at a fixed depth. Previous research prototypes fall short by presenting occlusion only for a fixed depth, and by presenting accommodation only for a narrow depth-range, or with poor depth or spatial resolution. This thesis explores a computational display approach, which is the co-design of the optics and the rendering pipeline, to address these challenges.  The approach for providing realistic accommodation is to synchronize a high-speed Digital Micro-Mirror (DMD) projector and a focus-tunable lens to periodically sweep out a volume of single-color binary images in-front of the user's eye. A novel voxel-oriented decomposition algorithm, and per-depth-plane illumination control enable full-color imagery presented over 280 depth planes refreshed at 60fps across a depth range of 15 - 400 cm. In a separate design, the approach for providing depth-dependent mutual occlusion is to design an optical system composed of focus-tunable lenses to realize a varifocal occlusion display capable of presenting imagery over a depth range of 30 - 300 cm.  Contributions of this thesis include new optical designs, new real-time rendering algorithms, and prototype displays that demonstrate accommodation and mutual occlusion depth cues over an extended depth-range. 


\end{abstract}
