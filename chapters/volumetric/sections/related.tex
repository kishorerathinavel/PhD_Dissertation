\section{Related Work}
\label{sec:volumetric:related_work}

\subsection{Volumetric Displays}
\label{sec:volumetric:volumetric_displays}

Volumetric displays create multiple real or virtual light sources in a three-dimensional volume of space and can typically be seen from a wide range of angles around the display. These light sources are the 3-D analog of pixels and are called \emph{voxels}. Earlier designs of volumetric displays were table-top designs and the displayed volume was confined to the \emph{physical volume of the display}~\cite{Favalora2002100, Sullivan2004Depthcube, Cossairt2007Geometric, Ochiai2016Fairy, Refai2009Static, Smalley2018photophoretic}. One of the limitations of most of these displays is that the light sources are presented additively and view-dependent effects, such as occlusion, are absent. This limitation is overcome in Cossairt et al.~\cite{Cossairt2007Geometric} and in Jones et al.~\cite{Jones2007Rendering} by using anisotropic diffusers.

Our proposed display provides a methodology to create virtual light sources over an \emph{extended volume external} to the display's physical volume. Applied to near-eye displays, this methodology has the potential to solve the vergence-accommodation conflict and reduces the need to track accommodation state in future eye-tracking technology. To clarify, our display needs eye-tracking in the sense that the \emph{pupil position} must be tracked, but the \emph{accommodation state} of the pupils need not be tracked.

\subsection{Accommodation supporting NEDs}
\label{sec:volumetric:accommodation_neds}
\subsubsection{Multifocal near-eye displays}
\label{sec:volumetric:multifocal_displays}

Multifocal near-eye displays, first proposed by Akeley et al.~\cite{Akeley2004stereo}, display a small number of images at different depths; the images are perceived additively~\cite{Akeley2004stereo,MacKenzie2010Accommodation,Liu2010novel,Love2009high,Hu2015Design}. In Akeley, et al.~\cite{Akeley2004stereo} and MacKenzie, et al.~\cite{MacKenzie2010Accommodation}, subregions of an LCD panel were mapped to different focal planes using beamsplitters. Liu and Hua~\cite{liu2009time}, Love et al.~\cite{Love2009high}, and Liu et al.~\cite{Liu2010novel} propose a switchable lens to multiplex between the multiple focal planes. Wang et al.~\cite{wang2018digitally} propose a segmented lens and a fast optical shutter to create the focal planes.  Hu and Hua~\cite{Hu2014design,Hu2014High,Hu2015Design} propose to use high-speed optical components, such as a DMD and a 1KHz deformable membrane mirror, to achieve a larger number of focal planes (six) than previously demonstrated. 

Because a relatively small number of depth planes are used to represent objects occupying a large volume, multifocal plane displays need scene decomposition algorithms to optimally represent a 3-D scene using a few 2-D image planes. Content generated by these scene decomposition algorithms provide synthetic focus cues to represent objects that lie in between the focal planes. MacKenzie et al.~\cite{MacKenzie2010Accommodation} propose a per-pixel linear blending approach. Narain et al.~\cite{Narain2015optimal} propose an optimized blending algorithm that can demonstrate occlusion, reflection, and non-Lambertian effects. Mercier et al.~\cite{Mercier2017Fast} and Lee et al.~\cite{Lee2017foveated} propose a new scene decomposition techniques that are tolerant to eye movements. While scene decomposition algorithms help to depict imagery that lie between the focal planes, the spatial frequency of the fused image is inversely related to the focal plane separation~\cite{Hu2014design, Hua2017Enabling}. 

Similar to multifocal displays, our display can also be thought of as a view-dependent and depth-fused multifocal display. Our display has about two orders of magnitude more focal planes than previous multifocal displays which approaches a \emph{volumetric display's} performance. Like previous multifocal displays, our display also requires eye-tracking to provide correct occlusion and dis-occlusion effects. In this paper, we assume that the pupil position is known. Like previous multifocal displays, we also share the problem of generating synthetic focus cues through scene decomposition to represent a large 3-D scene with 2-D image planes. However, while previous methods perform the scene decomposition in an image-oriented manner, we perform the scene decomposition in a voxel-oriented manner. This is discussed in detail in Section~\ref{sec:volumetric:rendering_pipeline}. 

Matsuda et al.~\cite{Matsuda2017focal} propose a multifocal display whose focal surfaces can acquire non-planar, scene-dependent surface geometry. Matsuda et al.~\cite{Matsuda2017focal} propose a rendering pipeline that converts a 3-D scene to multiple piecewise smooth 2-D surface representations that are displayed in a time-multiplex manner. In comparison with their work, our rendering pipeline generates a single 2-D surface representation of the 3-D scene, and our display does not require piecewise smooth 2-D surfaces. Our display also exhibits more uniform image quality throughout the displayed volume. 

Recently, Lee et al.~\cite{Lee2018Tomoreal,Lee2018Shape} propose a multifocal plane display which uses synchronized DMD, LCD panel, and focus-tunable lens. With the exception of their LCD panel and our HDR LEDs, the hardware and operation seem similar to our display. But, because of their use of LCD panel and our use of HDR LEDs, the rendering pipelines of the two displays are different. In their display, during the focus-tunable lens' cycle, the DMD panel is used to illuminate portions of the LCD panel resulting in color sub-images at various depths. In our display, during the focus-tunable lens' cycle, the HDR LEDs and DMD create a series of single-color binary images which integrate together such that a color volume is perceived. 


\subsubsection{Light field near-eye displays}
\label{sec:volumetric:light_field_displays}
Light field displays synthesize the individual light rays that recreate the 3-D scene and can conceptually provide accurate focus cues and monocular occlusion. However, current implementations of light field displays are diffraction-limited~\cite{Maimone2014Pinlight,Huang2015Light} or have poor resolution due to a spatial-angular resolution trade-off~\cite{Lanman2013near,Hua2014Three}. While light field displays present a virtual pixel by displaying the light rays originating from the virtual pixel individually, our volumetric NED displays the entire set of light rays that originate from the virtual pixel simultaneously.

\subsubsection{Holographic near-eye displays}
\label{sec:volumetric:holographic_displays}

Holographic displays precisely modulate the wave function of the image arriving at the pupil using a digital hologram displayed on a phase-only spatial light modulator (SLM) such as a phase-only liquid crystal on silicon (LCoS) panel. Conceptually, these displays can also provide accurate focus cues, monocular occlusion, vision correction, and non-Lambertian effects. Current implementations of holographic near-eye displays have a very small eyebox~\cite{Maimone2017Holographic}, and are computationally expensive~\cite{Shi2017Near,Maimone2017Holographic,Matsuda2017focal}. Our NED also has a small eyebox (4mm) and is moderately computationally intensive. Our NED's eyebox can be larger; the limiting factor for our eyebox is the focus-tunable lens's aperture (1cm). Maimone et al.~\cite{Maimone2017Holographic} demonstrate a NED that can provide per-pixel focus cues for a range of 10cm to 32.5cm. In comparison, our NED provides per-pixel near-accurate focus cues for a large depth range (15cm to 4M).

\subsubsection{Varifocal near-eye displays}
\label{sec:volumetric:varifocal_displays}
Varifocal near-eye displays have a single image plane where the vergence and focus cues match, and this plane is moved by using focus-tunable lenses~\cite{Padmanaban2016Optimizing,Liu2008Optical,Konrad2016Novel}, or deformable membrane mirrors~\cite{Dunn2017Wide}, or by actuating fixed-focus optical components~\cite{Aksit2017Near}. In a varifocal display, all pixels are at the same focal plane - so virtual pixels that do not lie on the plane of focus need to be synthetically blurred in proportion to their distance from the plane of focus. Varifocal displays need to track the accommodation state of the pupil~\cite{Padmanaban2016Optimizing} or assume that the pupils are accommodated to the eye convergence distance~\cite{Dunn2017Wide,Aksit2017Near}. 

\subsection{Rendering pipeline for DMD-based NEDs}
Previous NEDs have used DMDs and proposed different rendering pipelines~\cite{Lincoln2016motion,Lincoln2017scene,Hu2014design,Hu2015Design}. We build upon their hardware but propose a new rendering pipeline. A detailed discussion is provided in Section~\ref{sec:volumetric:previous_rendering_pipeline}.
