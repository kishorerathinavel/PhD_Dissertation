\section{Introduction}
\label{sec:volumetric:introduction}
Near-eye displays that seamlessly integrate virtual content into the real world offer exciting possibilities. Real-virtual integration could induce a paradigm shift in multiple aspects of our lives, including education, communication, entertainment, and others. Near-eye displays, as compared to spatially augmented reality and 3-D displays, allow true immersion in the sense that the near-eye display user could truly experience a virtual world around them in all directions while preserving the user's natural experience and view of the real world. However, several challenges must be addressed to realize truly immersive see-through near-eye displays. One of these is the mismatch between the vergence and accommodation cues of depth perception. Vergence is the orienting of our eyes such as to center the image of a fixated object on the fovea. Accommodation is the eye lenses' ability to change their focal length to bring the object of fixation into proper focus on the retina. These are cross-coupled physiological effects. Their absence, mismatch, or incorrect representation (may also apply to other depth cues) can disrupt the sense of presence or immersion and may cause visual discomfort, eyestrain, and nausea~\cite{Hoffman2008Vergence}. 

Some of the proposed solutions to the problem of providing such depth cues attempt to approximate focus cues. Varifocal displays, monovision displays, and even some implementations of multifocal displays are in this category. Some other proposed solutions, such as light field displays and holographic displays, provide accurate focus cues but have limitations. Current implementations of light field displays have poor resolution or are diffraction-limited. Current implementations of holographic displays are compute-intensive and may have very small eyeboxes. Phase-only spatial light modulator (SLM) technologies also need improvement before holographic displays based on these technologies can become practical.

This paper explores a new class of displays: volumetric near-eye displays. Our approach is to sweep the virtual image plane back and forth over a wide range of diopters and use high-speed DMDs coupled with high-speed illumination to present a large number of multiple thin slices of a computer generated volume. While this sounds similar to multifocal displays that show images at various fixed depths, there is a crucial difference:

Traditionally, for multifocal displays, the computer-generated volume is decomposed into a series of image planes placed at different depths; for time-multiplexed multifocal displays, this necessitates that the focus-tunable lens or deformable mirror settle down in each focus state. Our approach is to oscillate the focus-tunable lens in a continuous state and display a stack of binary images at high-speed such that the displayed stack of images is perceived as slices of a continuous full-color volume. We decompose the computer-generated volume locally, on a per-voxel basis, and distribute the decomposition around the location of the voxel. Thus, our rendering algorithm is aware of and leverages the fact that the focus-tunable lens is in continuous motion--rather than assuming a lens that moves and settles in discrete steps. Low-level hardware access to a high-speed DMD and a high-speed HDR RGB LED light source allows control of display pattern and illumination for each binary frame. We present a rendering pipeline for volumetric near-eye displays that utilize such hardware.

One might be concerned about the computational complexity of our approach. In our implementation, we make some simplifying assumptions to reduce computational overhead. However, these simplifications might not be desirable in a human-wearable product; without these assumptions, our approach would be moderately computationally demanding. While this might be an encumbrance for today's embedded hardware, we assert that near-eye displays (NEDs) of the future must have substantially more compute power to perform, e.g., low-latency corrections, head and eye tracking, real-world scene understanding, and so on. For example, onboard GPUs are already found in NEDs such as Microsoft HoloLens. While our current implementation is offline, we believe that future NEDs will have sufficient onboard computational resources to perform the required computations in real-time on the device.  
% * <blate@cs.unc.edu> 2018-06-19T15:37:20.027Z:
% 
% > moderately computationally demanding
% 
% Quantify, at least relative to the simplified approach... e.g., without these simplifications, computational requirements increase on the order of X percent. 
% 
% ^.

\subsection{Contributions}
\label{sec:Contributions}
This paper's main contributions are: 

\begin{enumerate}
\item A volumetric NED exhibiting 280 perceptually simultaneous binary depth planes, each an arbitrary RGB color, situated between 15cm (6.7 diopters) and 4M (0.25 diopters) from the viewer.
\item A rendering pipeline for the new NED that decomposes 3-D graphics primitives efficiently into the set of single-color binary depth planes, such that 24 bits-per-pixel color voxels can be displayed at 280 unique depth positions.
\end{enumerate}

\begin{comment}
\begin{enumerate}
\item We describe a new rendering pipeline for binary multifocal displays that allows a color voxel to be decomposed into a dense set of binary voxels in a small region around the color voxel. 
We reject the traditional notion that the color volume should be decomposed into a sparse set of color image planes, each located at a different depth. 
We analyze the losses in depth and spatial resolution that might result from the proposed decomposition and discuss mitigation methods.
\item We introduce volumetric NEDs capable of presenting high-resolution color imagery in a \emph{perceptually continuous} depth range across a large range of diopters. We demonstrate a prototype system that displays a 24-bits-per-pixel image in a binary volume composed of 280 focal planes distributed over a depth range of 15cm (6.7 diopters) to 4M (0.26.7 diopters), refreshed at 60 Hz. 
\end{enumerate}
\end{comment}

\subsection{Benefits}
\label{sec:benefits}
In addition to supporting the current volumetric display implementation, our proposed system can emulate varifocal displays and previous multifocal displays. This could allow the system to become a test-bed for future perceptual studies on accommodation. Our display allows low-level access to many stages of the graphics pipeline between GPU and the actual emission of light rays that form a retinal image. This low-level access could be used to study alternative rendering pipelines for future near-eye displays and advanced projectors. Integration of our present work and previous work with similar hardware~\cite{Lincoln2016motion,Lincoln2017scene} could lead to a near-eye display with several desirable properties (low-latency, high dynamic range, accommodation-capable). 

